{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "\n",
    "## Problem Description\n",
    "In this problem you will train decision tree and random forest models using sklearn on a real world dataset. The dataset is the *Cylinder Bands Data Set* from the UCI Machine Learning Repository: [https://archive.ics.uci.edu/ml/datasets/Cylinder+Bands](https://archive.ics.uci.edu/ml/datasets/Cylinder+Bands). The dataset is generated from rotogravure printers, with 39 unique features, and a binary classification label for each sample. The class is either 0, for 'band' or 1 for 'no band', where banding is an undesirable process delay that arises during the rotogravure printing process. By training ML models on this dataset, you could help identify or predict cases where these process delays are avoidable, thereby improving the efficiency of the printing. For the sake of this exercise, we only consider features 21-39 in the above link, and have removed any samples with missing values in that range. No further processing of the data is required on your behalf. The data has been partitioned into a training and testing set using an 80/20 split. Your models will be trained on just the train set, and accuracy results will be reported on both the training and testing sets.\n",
    "\n",
    "Fill out the notebook as instructed, making the requested plots and printing necessary values. \n",
    "\n",
    "*You are welcome to use any of the code provided in the lecture activities.*\n",
    "\n",
    "#### Summary of deliverables:\n",
    "\n",
    "- Accuracy function\n",
    "- Report accuracy of the DT model on the training and testing set\n",
    "- Report accuracy of the Random Forest model on the training and testing set\n",
    "\n",
    "#### Imports and Utility Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Use the `np.load()` function to load \"w5-hw1-train.npy\" (training data) and \"w5-hw1-test.npy\" (testing data). The first 19 columns of each are the features. The last column is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "train_data = np.load(\"data/w5-hw1-train.npy\")\n",
    "test_data = np.load(\"data/w5-hw1-test.npy\")\n",
    "\n",
    "X_train = train_data[:, :19]\n",
    "y_train = train_data[:, -1]\n",
    "\n",
    "X_test = test_data[:, :19]\n",
    "y_test = test_data[:, -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write an accuracy function\n",
    "\n",
    "Write a function `accuracy(pred,label)` that takes in the models prediction, and returns the percentage of predictions that match the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "def accuracy(pred, label):\n",
    "    acc = np.sum((pred == label)) * 100 / len(pred)\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a decision tree model\n",
    "\n",
    "Train a decision tree using `DecisionTreeClassifier()` with a `max_depth` of 10 and using a `random_state` of 0 to ensure repeatable results. Print the accuracy of the model on both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on training data:  93.12714776632302\n",
      "Model accuracy on test data:  65.75342465753425\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "model = DecisionTreeClassifier(max_depth = 10, random_state = 0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model accuracy of DecisionTreeClassifier on training data: \", accuracy(model.predict(X_train), y_train))\n",
    "print(\"Model accuracy of DecisionTreeClassifier on test data: \", accuracy(model.predict(X_test), y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a random forest model\n",
    "\n",
    "Train a random forest model using `RandomForestClassifier()` with a `max_depth` of 10, a `n_estimators` of 100, and using a random state of `0` to ensure repeatable results. Print the accuracy of the model on both the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of RandomForestClassifier on training data:  100.0\n",
      "Model accuracy of RandomForestClassifier on test data:  82.1917808219178\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "model_ = RandomForestClassifier(max_depth = 10, n_estimators = 100, random_state = 0)\n",
    "model_.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model accuracy of RandomForestClassifier on training data: \", accuracy(model_.predict(X_train), y_train))\n",
    "print(\"Model accuracy of RandomForestClassifier on test data: \", accuracy(model_.predict(X_test), y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the performance of the models\n",
    "\n",
    "Compare the training and testing accuracy of the two models, and explain why the random forest model is advantageous compared to a standard decision tree model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree might not be very accurate, especially if the depth is limited, there is noise in the data etc. Whereas in a random forest model, predictions of multiple decision trees are averaged out, resulting in irregularities being smoothened out and making the model less sensitive to noise and outliers. A decision might also be overfit to the data, which is avoided in random forest models, as the averaging and randomness generalizes the data better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
